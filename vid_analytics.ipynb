{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpJzwpYLHSAS",
        "outputId": "04be4376-5a1f-4a16-81d4-56d9ca406830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/898.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m890.9/898.5 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.5/898.5 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCreating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "[INFO] Using: cuda (Tesla T4)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 0) Install pinned deps (compat-safe)\n",
        "!pip -q install \"ultralytics==8.3.40\" deep_sort_realtime==1.3.2 \\\n",
        "                 \"opencv-python-headless==4.10.0.84\" torch torchvision \\\n",
        "                 \"transformers>=4.44.0\" timm accelerate ffmpeg-python\n",
        "\n",
        "# 1) Imports\n",
        "import os, sys, json, time, warnings, shutil, tempfile, subprocess, math\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "from google.colab import files\n",
        "from contextlib import nullcontext\n",
        "\n",
        "from transformers import (\n",
        "    SegformerImageProcessor, SegformerForSemanticSegmentation,\n",
        "    AutoImageProcessor, UperNetForSemanticSegmentation\n",
        ")\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# 2) User flags (stage toggles)\n",
        "RUN_DENOISING    = True\n",
        "RUN_TRACKING     = True\n",
        "RUN_OUTLINES     = True\n",
        "RUN_BG_ENSEMBLE  = True\n",
        "SAVE_TO_DRIVE    = False\n",
        "\n",
        "# 3) Config\n",
        "\n",
        "# CLAHE settings\n",
        "CLAHE_CLIP_LIMIT  = 2.0\n",
        "CLAHE_TILE_GRID   = (8, 8)\n",
        "\n",
        "# Denoising config (ONLY for detection frames)\n",
        "MEDIAN_KERNEL_SIZE = 5\n",
        "DENOISE_EVERY_NTH  = 1\n",
        "\n",
        "# Stage A (Tracking) config\n",
        "yolo_det_weights = \"yolov8s.pt\"\n",
        "yolo_conf        = 0.28\n",
        "yolo_iou         = 0.65\n",
        "yolo_imgsz       = 896\n",
        "working_width    = 960\n",
        "batch_size       = 24\n",
        "vid_stride       = 1\n",
        "\n",
        "# COCO subset for tracking\n",
        "COCO = {\n",
        "    \"person\":0, \"bicycle\":1, \"car\":2, \"motorcycle\":3, \"bus\":5, \"truck\":7,\n",
        "    \"bird\":14, \"cat\":15, \"dog\":16, \"horse\":17, \"sheep\":18, \"cow\":19\n",
        "}\n",
        "CLASS_FILTER = {\n",
        "    COCO[\"person\"],COCO[\"bicycle\"],COCO[\"car\"],COCO[\"motorcycle\"],\n",
        "    COCO[\"bus\"],COCO[\"truck\"],\n",
        "    COCO[\"cat\"],COCO[\"dog\"],COCO[\"horse\"],COCO[\"sheep\"],COCO[\"cow\"],COCO[\"bird\"]\n",
        "}\n",
        "\n",
        "# Colors for drawing tracked bounding boxes\n",
        "COLOR = {\n",
        "    \"person\": (0,220,0),\n",
        "    \"vehicle\": (0,160,255),\n",
        "    \"pet\": (255,120,0),\n",
        "    \"other\": (200,200,200)\n",
        "}\n",
        "\n",
        "# Stage B: outlines config\n",
        "yolo_seg_weights = \"yolov8x-seg.pt\"\n",
        "\n",
        "# Stage C: BG semantic ensemble config\n",
        "PROCESS_SCALE    = 0.6\n",
        "FRAME_STRIDE     = 1\n",
        "CONF_THRESH_BG   = 0.35\n",
        "MIN_AREA_FRAC    = 0.0010\n",
        "OPEN_K           = 3\n",
        "CLOSE_K          = 5\n",
        "MERGE_PAD_PX     = 3\n",
        "\n",
        "# Foreground suppression config\n",
        "DET_IMGSZ        = 960\n",
        "DET_CONF         = 0.25\n",
        "FG_CLASSES       = {0,1,2,3,5,7}\n",
        "FG_SUPPRESS_GAMMA = 2.0\n",
        "FG_SOFT_BLUR_SIGMA = 3.0\n",
        "\n",
        "# 4) Device sanity (GPU required)\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"CUDA not available. In Colab: Runtime -> Change runtime type -> GPU, then rerun.\")\n",
        "\n",
        "DEVICE_STR = \"cuda\"\n",
        "torch.cuda.set_device(0)\n",
        "\n",
        "try:\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "except Exception:\n",
        "    pass\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "cv2.setNumThreads(max(1, os.cpu_count() or 1))\n",
        "cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "print(f\"[INFO] Using: {DEVICE_STR} ({torch.cuda.get_device_name(0)})\")\n",
        "\n",
        "# # 5) Upload video\n",
        "# print(\"Choose a video file from your computer (mp4/mov/etc.)...\")\n",
        "# uploaded = files.upload()\n",
        "# if not uploaded:\n",
        "#     raise RuntimeError(\"No video uploaded.\")\n",
        "# INPUT_VIDEO = \"/content/\" + next(iter(uploaded.keys()))\n",
        "# print(\"[INFO] Uploaded:\", INPUT_VIDEO)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SHARED HELPERS + PREPROCESSING PIPELINE\n",
        "\n",
        "\n",
        "def class_bucket(cid:int)->str:\n",
        "    if cid == COCO[\"person\"]:\n",
        "        return \"person\"\n",
        "    if cid in {COCO[\"bicycle\"],COCO[\"car\"],COCO[\"motorcycle\"],COCO[\"bus\"],COCO[\"truck\"]}:\n",
        "        return \"vehicle\"\n",
        "    if cid in {COCO[\"cat\"],COCO[\"dog\"],COCO[\"horse\"],COCO[\"sheep\"],COCO[\"cow\"],COCO[\"bird\"]}:\n",
        "        return \"pet\"\n",
        "    return \"other\"\n",
        "\n",
        "def draw_track(frame, tlbr, track_id, bucket):\n",
        "    x1,y1,x2,y2 = map(int, tlbr)\n",
        "    c = COLOR.get(bucket, COLOR[\"other\"])\n",
        "    cv2.rectangle(frame, (x1,y1), (x2,y2), c, 2)\n",
        "    cv2.putText(frame, f\"{bucket} #{track_id}\", (x1, max(0,y1-6)),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, c, 2)\n",
        "\n",
        "def resize_keep_aspect(img, target_w):\n",
        "    h,w = img.shape[:2]\n",
        "    if not target_w or w == target_w:\n",
        "        return img, (w,h)\n",
        "    s = float(target_w)/float(w)\n",
        "    new_w = target_w\n",
        "    new_h = int(round(h*s))\n",
        "    return cv2.resize(img, (new_w,new_h), interpolation=cv2.INTER_AREA), (new_w,new_h)\n",
        "\n",
        "def apply_clahe_bgr(bgr: np.ndarray,\n",
        "                    clip_limit: float = CLAHE_CLIP_LIMIT,\n",
        "                    grid_size: Tuple[int,int] = CLAHE_TILE_GRID) -> np.ndarray:\n",
        "    lab = cv2.cvtColor(bgr, cv2.COLOR_BGR2LAB)\n",
        "    l,a,b = cv2.split(lab)\n",
        "    l = cv2.createCLAHE(clipLimit=float(clip_limit),\n",
        "                        tileGridSize=(int(grid_size[0]), int(grid_size[1]))).apply(l)\n",
        "    return cv2.cvtColor(cv2.merge([l,a,b]), cv2.COLOR_LAB2BGR)\n",
        "\n",
        "def ffmpeg_to_h264(inp, outp, fps):\n",
        "    subprocess.run(\n",
        "        [\"ffmpeg\",\"-y\",\"-i\",inp,\"-movflags\",\"+faststart\",\"-vcodec\",\"libx264\",\n",
        "         \"-pix_fmt\",\"yuv420p\",\"-r\",f\"{fps:.3f}\",\"-preset\",\"veryfast\",outp],\n",
        "        stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
        "    )\n",
        "\n",
        "def morph(mask, open_k=0, close_k=0, dilate_k=0):\n",
        "    if open_k and open_k>1:\n",
        "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN,\n",
        "                                cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(open_k,open_k)))\n",
        "    if close_k and close_k>1:\n",
        "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE,\n",
        "                                cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(close_k,close_k)))\n",
        "    if dilate_k and dilate_k>0:\n",
        "        mask = cv2.dilate(mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(dilate_k,dilate_k)), iterations=1)\n",
        "    return mask\n",
        "\n",
        "def apply_median_denoise(frame, kernel_size=5):\n",
        "    return cv2.medianBlur(frame, int(kernel_size))\n",
        "\n",
        "# frame utilities\n",
        "def _save_frame_at_ratio(video_path: str, out_path: str, ratio: float = 0.5) -> bool:\n",
        "    try:\n",
        "        if not os.path.exists(video_path) or os.path.getsize(video_path) < 1000:\n",
        "            return False\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return False\n",
        "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "        if total <= 0:\n",
        "            cap.release()\n",
        "            return False\n",
        "        r = min(max(ratio, 0.0), 1.0)\n",
        "        idx = min(max(int(round((total - 1) * r)), 0), max(total - 1, 0))\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ok, frame = cap.read()\n",
        "        cap.release()\n",
        "        if not ok or frame is None:\n",
        "            return False\n",
        "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "        return cv2.imwrite(out_path, frame)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def save_best_tracking_paper_frame(\n",
        "    out_path: str = \"/content/paper_frame_tracking.jpg\",\n",
        "    prefer_list: List[str] = [\n",
        "        \"/content/debug_frame.jpg\",\n",
        "        \"/content/debug_frame_midpoint.jpg\",\n",
        "        \"/content/debug_frame_end.jpg\",\n",
        "    ],\n",
        "    fallback_video: str = \"/content/output_tracked.mp4\",\n",
        "    fallback_ratio: float = 0.5,\n",
        ") -> bool:\n",
        "    for p in prefer_list:\n",
        "        if os.path.exists(p) and os.path.getsize(p) > 1000:\n",
        "            try:\n",
        "                img = cv2.imread(p)\n",
        "                if img is not None:\n",
        "                    return cv2.imwrite(out_path, img)\n",
        "            except Exception:\n",
        "                pass\n",
        "    return _save_frame_at_ratio(fallback_video, out_path, ratio=fallback_ratio)\n",
        "\n",
        "# Denoising stage\n",
        "def run_video_denoising(input_path: str, output_path: str):\n",
        "    print(\"\\n[DENOISING] Enabled internally for detection only (median blur).\")\n",
        "    print(\"[DENOISING] Outputs remain sharp; file copy is used here.\")\n",
        "    shutil.copy(input_path, output_path)\n",
        "    return output_path\n",
        "\n",
        "print(\"[INFO] Helper functions loaded successfully\")\n"
      ],
      "metadata": {
        "id": "NixroP5aLoH6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "316b45a4-ffc5-4451-8a92-bf75d31649ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Helper functions loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STAGE A (YOLOv8 + DEEPSORT TRACKING)\n",
        "\n",
        "def run_tracking_fullvideo(input_path:str, save_to_drive:bool=False):\n",
        "    COCO_FILTER_SORTED = sorted(list(CLASS_FILTER))\n",
        "\n",
        "    model = YOLO(yolo_det_weights)\n",
        "\n",
        "    tracker = DeepSort(\n",
        "        max_age=60, n_init=3, max_iou_distance=0.7, nms_max_overlap=1.0,\n",
        "        embedder=\"mobilenet\", embedder_gpu=True, half=True, bgr=True\n",
        "    )\n",
        "\n",
        "    cap_probe = cv2.VideoCapture(input_path)\n",
        "    if not cap_probe.isOpened():\n",
        "        raise RuntimeError(\"Cannot open video (probe).\")\n",
        "\n",
        "    fps_in = cap_probe.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "    total_frames = int(cap_probe.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    ok, first_frame = cap_probe.read()\n",
        "    if not ok:\n",
        "        raise RuntimeError(\"Cannot read first frame (probe).\")\n",
        "    cap_probe.release()\n",
        "\n",
        "    tmpdir = Path(tempfile.mkdtemp(prefix=\"yolo_track_\"))\n",
        "    local_in = str(tmpdir / Path(input_path).name)\n",
        "    if os.path.abspath(local_in) != os.path.abspath(input_path):\n",
        "        shutil.copy(input_path, local_in)\n",
        "    else:\n",
        "        local_in = input_path\n",
        "\n",
        "    cap = cv2.VideoCapture(local_in)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(\"Cannot open video (processing).\")\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "    ok, fr0 = cap.read()\n",
        "    if not ok:\n",
        "        raise RuntimeError(\"Cannot read first frame (processing).\")\n",
        "\n",
        "    fr0s, (W,H) = resize_keep_aspect(fr0, working_width)\n",
        "\n",
        "    raw_out_path = str(tmpdir / \"temp_raw.mp4\")\n",
        "    writer_raw = cv2.VideoWriter(\n",
        "        raw_out_path, cv2.VideoWriter_fourcc(*\"mp4v\"), max(1.0, fps), (W,H)\n",
        "    )\n",
        "\n",
        "    _ = model.predict(\n",
        "        source=[np.zeros((int(H),int(W),3), np.uint8)],\n",
        "        conf=yolo_conf, iou=yolo_iou, imgsz=yolo_imgsz,\n",
        "        device=\"cuda\", half=True, batch=1, verbose=False, classes=COCO_FILTER_SORTED\n",
        "    )\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "\n",
        "    batch_bgr_detect = []\n",
        "    batch_bgr_out    = []\n",
        "\n",
        "    frames_written = 0\n",
        "    detections_total = 0\n",
        "    frame_idx = 0\n",
        "\n",
        "    valid_debug_frame_saved = False\n",
        "    midpoint_frame_index = max(1, total_frames//2)\n",
        "\n",
        "    start_t = time.time()\n",
        "\n",
        "    def run_yolo(frames_bgr):\n",
        "        if not frames_bgr:\n",
        "            return []\n",
        "        return model.predict(\n",
        "            source=frames_bgr, conf=yolo_conf, iou=yolo_iou, imgsz=yolo_imgsz,\n",
        "            device=\"cuda\", half=True, batch=len(frames_bgr), verbose=False, classes=COCO_FILTER_SORTED\n",
        "        )\n",
        "\n",
        "    def save_debug_frame(img_bgr, out_path):\n",
        "        try:\n",
        "            cv2.imwrite(out_path, img_bgr)\n",
        "            print(\"[INFO] Saved debug frame to\", out_path)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    while True:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok:\n",
        "            break\n",
        "\n",
        "        if (frame_idx % vid_stride) != 0:\n",
        "            frame_idx += 1\n",
        "            continue\n",
        "\n",
        "        resized_original, _ = resize_keep_aspect(frame, working_width)\n",
        "        out_frame = apply_clahe_bgr(resized_original)\n",
        "\n",
        "        det_src = frame\n",
        "        if RUN_DENOISING and (DENOISE_EVERY_NTH <= 1 or (frame_idx % int(DENOISE_EVERY_NTH) == 0)):\n",
        "            det_src = apply_median_denoise(det_src, MEDIAN_KERNEL_SIZE)\n",
        "\n",
        "        resized_det, _ = resize_keep_aspect(det_src, working_width)\n",
        "        det_frame = apply_clahe_bgr(resized_det)\n",
        "\n",
        "        batch_bgr_detect.append(det_frame)\n",
        "        batch_bgr_out.append(out_frame)\n",
        "\n",
        "        if len(batch_bgr_detect) >= batch_size:\n",
        "            results = run_yolo(batch_bgr_detect)\n",
        "\n",
        "            for bi, r in enumerate(results):\n",
        "                f_bgr = batch_bgr_out[bi]\n",
        "                f_bgr_detect = batch_bgr_detect[bi]\n",
        "                had_det = False\n",
        "\n",
        "                if getattr(r, \"boxes\", None) is not None and len(r.boxes) > 0:\n",
        "                    boxes = r.boxes.xyxy.detach().cpu().numpy()\n",
        "                    confs = r.boxes.conf.detach().cpu().numpy()\n",
        "                    clses = r.boxes.cls.detach().cpu().numpy().astype(int)\n",
        "\n",
        "                    det = []\n",
        "                    for (x1,y1,x2,y2), cf, cid in zip(boxes, confs, clses):\n",
        "                        if cf < yolo_conf or cid not in CLASS_FILTER:\n",
        "                            continue\n",
        "                        det.append(([float(x1), float(y1), float(x2-x1), float(y2-y1)], float(cf), int(cid)))\n",
        "\n",
        "                    tracks = tracker.update_tracks(det, frame=f_bgr_detect)\n",
        "                    detections_total += len(det)\n",
        "\n",
        "                    for t in tracks:\n",
        "                        if not t.is_confirmed() or t.time_since_update > 0:\n",
        "                            continue\n",
        "                        det_cls = getattr(t, \"det_class\", getattr(t, \"cls\", COCO[\"person\"]))\n",
        "                        draw_track(f_bgr, t.to_ltrb(), t.track_id, class_bucket(int(det_cls)))\n",
        "                        had_det = True\n",
        "\n",
        "                writer_raw.write(f_bgr)\n",
        "                frames_written += 1\n",
        "\n",
        "                if had_det and (not valid_debug_frame_saved):\n",
        "                    save_debug_frame(f_bgr, \"/content/debug_frame.jpg\")\n",
        "                    valid_debug_frame_saved = True\n",
        "\n",
        "                if (not valid_debug_frame_saved) and (frame_idx >= midpoint_frame_index):\n",
        "                    save_debug_frame(f_bgr, \"/content/debug_frame_midpoint.jpg\")\n",
        "\n",
        "            batch_bgr_detect.clear()\n",
        "            batch_bgr_out.clear()\n",
        "\n",
        "        frame_idx += 1\n",
        "\n",
        "    if batch_bgr_detect:\n",
        "        results = run_yolo(batch_bgr_detect)\n",
        "        for bi, r in enumerate(results):\n",
        "            f_bgr = batch_bgr_out[bi]\n",
        "            f_bgr_detect = batch_bgr_detect[bi]\n",
        "            had_det = False\n",
        "\n",
        "            if getattr(r, \"boxes\", None) is not None and len(r.boxes) > 0:\n",
        "                boxes = r.boxes.xyxy.detach().cpu().numpy()\n",
        "                confs = r.boxes.conf.detach().cpu().numpy()\n",
        "                clses = r.boxes.cls.detach().cpu().numpy().astype(int)\n",
        "\n",
        "                det=[]\n",
        "                for (x1,y1,x2,y2), cf, cid in zip(boxes, confs, clses):\n",
        "                    if cf < yolo_conf or cid not in CLASS_FILTER:\n",
        "                        continue\n",
        "                    det.append(([float(x1), float(y1), float(x2-x1), float(y2-y1)], float(cf), int(cid)))\n",
        "\n",
        "                tracks = tracker.update_tracks(det, frame=f_bgr_detect)\n",
        "                detections_total += len(det)\n",
        "                for t in tracks:\n",
        "                    if not t.is_confirmed() or t.time_since_update > 0:\n",
        "                        continue\n",
        "                    det_cls = getattr(t, \"det_class\", getattr(t, \"cls\", COCO[\"person\"]))\n",
        "                    draw_track(f_bgr, t.to_ltrb(), t.track_id, class_bucket(int(det_cls)))\n",
        "                    had_det = True\n",
        "\n",
        "            writer_raw.write(f_bgr)\n",
        "            frames_written += 1\n",
        "\n",
        "            if had_det and (not valid_debug_frame_saved):\n",
        "                save_debug_frame(f_bgr, \"/content/debug_frame.jpg\")\n",
        "                valid_debug_frame_saved = True\n",
        "\n",
        "            if (not valid_debug_frame_saved) and (bi == len(results) - 1):\n",
        "                save_debug_frame(f_bgr, \"/content/debug_frame_end.jpg\")\n",
        "\n",
        "    cap.release()\n",
        "    writer_raw.release()\n",
        "\n",
        "    final_path = \"/content/output_tracked.mp4\"\n",
        "    ffmpeg_to_h264(raw_out_path, final_path, fps_in)\n",
        "\n",
        "    drive_copy = None\n",
        "    if save_to_drive:\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount(\"/content/drive\", force_remount=False)\n",
        "            drive_dir = Path(\"/content/drive/MyDrive/colab_videos\")\n",
        "            drive_dir.mkdir(parents=True, exist_ok=True)\n",
        "            drive_copy = str(drive_dir / Path(final_path).name)\n",
        "            shutil.copy(final_path, drive_copy)\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Drive copy failed:\", e)\n",
        "\n",
        "    elapsed = time.time() - start_t\n",
        "    eff_fps = frames_written / max(1e-6, elapsed)\n",
        "\n",
        "    stats = {\n",
        "        \"device\": DEVICE_STR,\n",
        "        \"gpu_name\": torch.cuda.get_device_name(0),\n",
        "        \"mode_used\": \"paper_clahe + denoise_for_detection_only\",\n",
        "        \"vid_stride_used\": vid_stride,\n",
        "        \"frames_written\": frames_written,\n",
        "        \"detections_total\": detections_total,\n",
        "        \"elapsed_sec_total_pipeline\": round(elapsed, 2),\n",
        "        \"effective_processing_fps\": round(eff_fps, 2),\n",
        "        \"input_fps\": fps_in,\n",
        "        \"input_frames\": total_frames,\n",
        "        \"writer_fps\": fps_in,\n",
        "        \"yolo_weights\": yolo_det_weights,\n",
        "        \"yolo_conf\": yolo_conf,\n",
        "        \"yolo_imgsz\": yolo_imgsz,\n",
        "        \"output_file\": final_path,\n",
        "        \"debug_frame_primary\": \"/content/debug_frame.jpg\",\n",
        "        \"debug_frame_mid\": \"/content/debug_frame_midpoint.jpg\",\n",
        "        \"debug_frame_end\": \"/content/debug_frame_end.jpg\",\n",
        "        \"drive_copy\": drive_copy,\n",
        "        \"clahe_enabled\": True,\n",
        "        \"clahe_clip_limit\": CLAHE_CLIP_LIMIT,\n",
        "        \"clahe_tile_grid\": CLAHE_TILE_GRID,\n",
        "        \"denoising_enabled_for_detection\": bool(RUN_DENOISING),\n",
        "        \"median_kernel_size\": int(MEDIAN_KERNEL_SIZE),\n",
        "    }\n",
        "    with open(\"/content/run_stats.json\",\"w\") as f:\n",
        "        json.dump(stats, f, indent=2)\n",
        "\n",
        "    return stats\n"
      ],
      "metadata": {
        "id": "r9ACFCrH4OWh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STAGE B (SEGMENTATION OUTLINES)\n",
        "\n",
        "\n",
        "def run_outlines(input_path:str, out_path:str):\n",
        "    model = YOLO(yolo_seg_weights)\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(\"Could not open video for outlines.\")\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    writer = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w,h))\n",
        "\n",
        "    with tqdm(total=total_frames, desc=\"Outlines\", unit=\"frame\") as pbar:\n",
        "        while True:\n",
        "            ok, frame = cap.read()\n",
        "            if not ok:\n",
        "                break\n",
        "\n",
        "            frame_vis = apply_clahe_bgr(frame)\n",
        "\n",
        "            res = model.predict(frame_vis, verbose=False, device=\"cuda\")\n",
        "            r = res[0]\n",
        "\n",
        "            masks_xy = []\n",
        "\n",
        "            if getattr(r, \"masks\", None) is not None:\n",
        "                if getattr(r.masks, \"xy\", None) is not None:\n",
        "                    masks_xy = r.masks.xy\n",
        "\n",
        "                elif getattr(r.masks, \"data\", None) is not None:\n",
        "                    H,W = frame_vis.shape[:2]\n",
        "                    for mi in r.masks.data.detach().cpu().numpy():\n",
        "                        m = (mi>0.5).astype(np.uint8)*255\n",
        "                        m = cv2.resize(m,(W,H), interpolation=cv2.INTER_NEAREST)\n",
        "                        cnts,_ = cv2.findContours(m, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "                        for c in cnts:\n",
        "                            masks_xy.append(c.reshape(-1,2))\n",
        "\n",
        "            for pts in masks_xy:\n",
        "                pts = np.asarray(pts, dtype=np.int32)\n",
        "                if pts.ndim == 2:\n",
        "                    pts = pts.reshape(-1,1,2)\n",
        "                cv2.polylines(frame_vis, [pts], isClosed=True, color=(0,255,0), thickness=2)\n",
        "\n",
        "            writer.write(frame_vis)\n",
        "            pbar.update(1)\n",
        "\n",
        "    cap.release()\n",
        "    writer.release()\n",
        "    print(\"[OK] Outlines saved ->\", out_path)\n"
      ],
      "metadata": {
        "id": "NQmD6dQ8F7Hd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#STAGE C + FINAL RUNNER\n",
        "\n",
        "class ADE20KEnsembler:\n",
        "    def __init__(self, device=\"cuda\"):\n",
        "        self.device = device\n",
        "\n",
        "        self.proc1 = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
        "        self.m1    = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\").to(device).eval()\n",
        "\n",
        "        self.proc2 = AutoImageProcessor.from_pretrained(\"openmmlab/upernet-convnext-tiny\")\n",
        "        self.m2    = UperNetForSemanticSegmentation.from_pretrained(\"openmmlab/upernet-convnext-tiny\").to(device).eval()\n",
        "\n",
        "        self.id2label_1 = self.m1.config.id2label\n",
        "        self.id2label_2 = self.m2.config.id2label\n",
        "\n",
        "        def ids_for(tokens, id2label):\n",
        "            s=set()\n",
        "            for cid,name in id2label.items():\n",
        "                n=str(name).lower()\n",
        "                if any(tok in n for tok in tokens):\n",
        "                    s.add(int(cid))\n",
        "            return s\n",
        "\n",
        "        water_tokens = {\"water\",\"river\",\"sea\",\"ocean\",\"lake\",\"pond\",\"canal\",\"fountain\",\"waterfall\",\"pool\",\"swimming\"}\n",
        "        veg_tokens   = {\"tree\",\"grass\",\"plant\",\"vegetation\",\"forest\",\"bush\",\"shrub\",\"hedge\",\"branch\",\"palm\",\"leaf\",\"leaves\",\"meadow\",\"field\"}\n",
        "        sky_tokens   = {\"sky\",\"cloud\",\"clouds\"}\n",
        "        building_tokens = {\"building\",\"house\",\"skyscraper\",\"edifice\",\"tower\",\"structure\",\"construction\",\"architecture\"}\n",
        "        road_tokens  = {\"road\",\"street\",\"path\",\"highway\",\"pavement\",\"sidewalk\",\"lane\",\"asphalt\",\"pathway\"}\n",
        "        grass_tokens = {\"grass\",\"lawn\",\"turf\",\"meadow\",\"field\"}\n",
        "\n",
        "        self.water_ids_1 = ids_for(water_tokens, self.id2label_1)\n",
        "        self.veg_ids_1   = ids_for(veg_tokens,   self.id2label_1)\n",
        "        self.sky_ids_1   = ids_for(sky_tokens,   self.id2label_1)\n",
        "        self.building_ids_1 = ids_for(building_tokens, self.id2label_1)\n",
        "        self.road_ids_1  = ids_for(road_tokens,  self.id2label_1)\n",
        "        self.grass_ids_1 = ids_for(grass_tokens, self.id2label_1)\n",
        "\n",
        "        self.water_ids_2 = ids_for(water_tokens, self.id2label_2)\n",
        "        self.veg_ids_2   = ids_for(veg_tokens,   self.id2label_2)\n",
        "        self.sky_ids_2   = ids_for(sky_tokens,   self.id2label_2)\n",
        "        self.building_ids_2 = ids_for(building_tokens, self.id2label_2)\n",
        "        self.road_ids_2  = ids_for(road_tokens,  self.id2label_2)\n",
        "        self.grass_ids_2 = ids_for(grass_tokens, self.id2label_2)\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def predict_probs(self, bgr: np.ndarray, amp=True):\n",
        "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        inp1 = self.proc1(images=rgb, return_tensors=\"pt\").to(self.device)\n",
        "        inp2 = self.proc2(images=rgb, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        autocast_ctx = torch.cuda.amp.autocast if (self.device==\"cuda\" and amp) else nullcontext\n",
        "        with autocast_ctx(dtype=torch.float16) if (self.device==\"cuda\" and amp) else nullcontext():\n",
        "            out1 = self.m1(**inp1).logits\n",
        "            out2 = self.m2(**inp2).logits\n",
        "\n",
        "        H,W = rgb.shape[:2]\n",
        "\n",
        "        p1 = nn.functional.interpolate(out1, size=(H,W), mode=\"bilinear\", align_corners=False).float().softmax(dim=1)[0]\n",
        "        p2 = nn.functional.interpolate(out2, size=(H,W), mode=\"bilinear\", align_corners=False).float().softmax(dim=1)[0]\n",
        "\n",
        "        def sum_probs(pmap, ids:set):\n",
        "            if not ids:\n",
        "                return torch.zeros_like(pmap[0])\n",
        "            idx = torch.tensor(sorted(list(ids)), device=pmap.device, dtype=torch.long)\n",
        "            idx = idx[(idx>=0) & (idx<pmap.shape[0])]\n",
        "            return pmap.index_select(0, idx).sum(dim=0) if idx.numel() else torch.zeros_like(pmap[0])\n",
        "\n",
        "        water_p    = (sum_probs(p1,self.water_ids_1)    + sum_probs(p2,self.water_ids_2))    * 0.5\n",
        "        veg_p      = (sum_probs(p1,self.veg_ids_1)      + sum_probs(p2,self.veg_ids_2))      * 0.5\n",
        "        sky_p      = (sum_probs(p1,self.sky_ids_1)      + sum_probs(p2,self.sky_ids_2))      * 0.5\n",
        "        building_p = (sum_probs(p1,self.building_ids_1) + sum_probs(p2,self.building_ids_2)) * 0.5\n",
        "        road_p     = (sum_probs(p1,self.road_ids_1)     + sum_probs(p2,self.road_ids_2))     * 0.5\n",
        "        grass_p    = (sum_probs(p1,self.grass_ids_1)    + sum_probs(p2,self.grass_ids_2))    * 0.5\n",
        "\n",
        "        return (water_p.detach().cpu().numpy(),\n",
        "                veg_p.detach().cpu().numpy(),\n",
        "                sky_p.detach().cpu().numpy(),\n",
        "                building_p.detach().cpu().numpy(),\n",
        "                road_p.detach().cpu().numpy(),\n",
        "                grass_p.detach().cpu().numpy())\n",
        "\n",
        "class ForegroundSuppressor:\n",
        "    def __init__(self, device=\"cuda\"):\n",
        "        self.model = YOLO(\"yolov8n-seg.pt\")\n",
        "        self.device = 0 if device==\"cuda\" else \"cpu\"\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def get_fg_mask(self, frame_bgr: np.ndarray, classes=FG_CLASSES, imgsz=DET_IMGSZ, conf=DET_CONF):\n",
        "        H,W = frame_bgr.shape[:2]\n",
        "        res = self.model.predict(frame_bgr, imgsz=imgsz, conf=conf, device=self.device, verbose=False)\n",
        "        if not res or len(res)==0:\n",
        "            return np.zeros((H,W), np.uint8)\n",
        "        r = res[0]\n",
        "        if r.masks is None or r.boxes is None:\n",
        "            return np.zeros((H,W), np.uint8)\n",
        "\n",
        "        clses = r.boxes.cls.cpu().numpy().astype(int)\n",
        "\n",
        "        if getattr(r.masks, \"xy\", None) is not None:\n",
        "            canvas = np.zeros((H,W), np.uint8)\n",
        "            for poly, cls_id in zip(r.masks.xy, clses):\n",
        "                if cls_id not in classes:\n",
        "                    continue\n",
        "                pts = np.asarray(poly, dtype=np.int32)\n",
        "                if pts.ndim == 2:\n",
        "                    pts = pts.reshape(-1,1,2)\n",
        "                cv2.fillPoly(canvas, [pts], 255)\n",
        "            k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))\n",
        "            return cv2.morphologyEx(canvas, cv2.MORPH_CLOSE, k, iterations=1)\n",
        "\n",
        "        data = []\n",
        "        if getattr(r.masks, \"data\", None) is not None:\n",
        "            data = r.masks.data.cpu().numpy()\n",
        "\n",
        "        fg = np.zeros((H,W), np.uint8)\n",
        "        for mi, cls_id in zip(data, clses):\n",
        "            if cls_id not in classes:\n",
        "                continue\n",
        "            mask = (mi>0.5).astype(np.uint8)*255\n",
        "            mask = cv2.resize(mask,(W,H), interpolation=cv2.INTER_NEAREST)\n",
        "            fg = np.maximum(fg, mask)\n",
        "\n",
        "        k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))\n",
        "        return cv2.morphologyEx(fg, cv2.MORPH_CLOSE, k, iterations=1)\n",
        "\n",
        "def run_bg_ensemble(input_path:str, out_path:str):\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(\"Could not open video\")\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "    W0 = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    H0 = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    Wp = max(64, int(W0*PROCESS_SCALE))\n",
        "    Hp = max(64, int(H0*PROCESS_SCALE))\n",
        "\n",
        "    writer = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (W0,H0))\n",
        "\n",
        "    ens = ADE20KEnsembler(device=\"cuda\")\n",
        "    fgnet = ForegroundSuppressor(device=\"cuda\")\n",
        "\n",
        "    def draw_label_smart(img, x1,y1,x2,y2, text, color, font_scale=0.55, thick=2, pad=6):\n",
        "        (tw,th),_ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thick)\n",
        "        H,W = img.shape[:2]\n",
        "\n",
        "        ay2 = y1 - pad\n",
        "        ay1 = ay2 - th - pad\n",
        "        if ay1 >= 0:\n",
        "            bx1 = max(0, min(W-(tw+2*pad), x1)); bx2 = min(W, bx1+tw+2*pad)\n",
        "            cv2.rectangle(img, (bx1,ay1), (bx2,ay2), color, -1)\n",
        "            cv2.putText(img, text, (bx1+pad, ay2-pad), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0,0,0), thick)\n",
        "            return\n",
        "\n",
        "        by1 = y2 + pad\n",
        "        by2 = by1 + th + pad\n",
        "        if by2 <= H:\n",
        "            bx1 = max(0, min(W-(tw+2*pad), x1)); bx2 = min(W, bx1+tw+2*pad)\n",
        "            cv2.rectangle(img, (bx1,by1), (bx2,by2), color, -1)\n",
        "            cv2.putText(img, text, (bx1+pad, by2-pad), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0,0,0), thick)\n",
        "            return\n",
        "\n",
        "        rx1 = x2 + pad\n",
        "        rx2 = rx1 + tw + 2*pad\n",
        "        if rx2 > W:\n",
        "            shift = rx2 - W\n",
        "            rx1 -= shift; rx2 -= shift\n",
        "            rx1 = max(0, rx1)\n",
        "        ry1 = int((y1 + y2 - th - 2*pad) / 2)\n",
        "        ry1 = max(0, min(H - (th + 2*pad), ry1))\n",
        "        ry2 = ry1 + th + 2*pad\n",
        "        cv2.rectangle(img,(rx1,ry1),(rx2,ry2), color, -1)\n",
        "        cv2.putText(img, text, (rx1+pad, ry2-pad), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0,0,0), thick)\n",
        "\n",
        "    i = 0\n",
        "    t0 = time.time()\n",
        "\n",
        "    with tqdm(total=total_frames, desc=\"BG Ensemble\", unit=\"frame\") as pbar:\n",
        "        while True:\n",
        "            ok, frame_full = cap.read()\n",
        "            if not ok:\n",
        "                break\n",
        "\n",
        "            if (i % FRAME_STRIDE) != 0:\n",
        "                i += 1\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "\n",
        "            frame_full = apply_clahe_bgr(frame_full)\n",
        "\n",
        "            fg_mask_u8 = fgnet.get_fg_mask(frame_full)\n",
        "            fg_prob = fg_mask_u8.astype(np.float32) / 255.0\n",
        "\n",
        "            if FG_SOFT_BLUR_SIGMA and FG_SOFT_BLUR_SIGMA > 0:\n",
        "                fg_prob = cv2.GaussianBlur(fg_prob, (0,0), float(FG_SOFT_BLUR_SIGMA))\n",
        "                fg_prob = np.clip(fg_prob, 0.0, 1.0)\n",
        "\n",
        "            suppress_factor = np.power(1.0 - fg_prob, float(FG_SUPPRESS_GAMMA))\n",
        "\n",
        "            frame_proc = cv2.resize(frame_full, (Wp,Hp), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "            try:\n",
        "                pw, pv, ps, pb, pr, pg = ens.predict_probs(frame_proc, amp=True)\n",
        "            except torch.cuda.OutOfMemoryError:\n",
        "                print(\"[WARN] CUDA OOM -> ensemble on CPU...\")\n",
        "                ens.device = \"cpu\"; ens.m1=ens.m1.to(\"cpu\"); ens.m2=ens.m2.to(\"cpu\")\n",
        "                pw, pv, ps, pb, pr, pg = ens.predict_probs(frame_proc, amp=False)\n",
        "\n",
        "            pw = cv2.resize(pw,(W0,H0), interpolation=cv2.INTER_LINEAR)\n",
        "            pv = cv2.resize(pv,(W0,H0), interpolation=cv2.INTER_LINEAR)\n",
        "            ps = cv2.resize(ps,(W0,H0), interpolation=cv2.INTER_LINEAR)\n",
        "            pb = cv2.resize(pb,(W0,H0), interpolation=cv2.INTER_LINEAR)\n",
        "            pr = cv2.resize(pr,(W0,H0), interpolation=cv2.INTER_LINEAR)\n",
        "            pg = cv2.resize(pg,(W0,H0), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "            pw *= suppress_factor\n",
        "            pv *= suppress_factor\n",
        "            ps *= suppress_factor\n",
        "            pb *= suppress_factor\n",
        "            pr *= suppress_factor\n",
        "            pg *= suppress_factor\n",
        "\n",
        "            probs = np.stack([pw,pv,ps,pb,pr,pg], axis=0)\n",
        "            maxprob = probs.max(axis=0)\n",
        "            cls = probs.argmax(axis=0).astype(np.int32)\n",
        "            cls[maxprob < CONF_THRESH_BG] = 6\n",
        "\n",
        "            mw = (cls==0).astype(np.uint8)*255\n",
        "            mv = (cls==1).astype(np.uint8)*255\n",
        "            ms = (cls==2).astype(np.uint8)*255\n",
        "            mb = (cls==3).astype(np.uint8)*255\n",
        "            mr = (cls==4).astype(np.uint8)*255\n",
        "            mg = (cls==5).astype(np.uint8)*255\n",
        "\n",
        "            mw = morph(mw, OPEN_K, CLOSE_K, MERGE_PAD_PX)\n",
        "            mv = morph(mv, OPEN_K, CLOSE_K, MERGE_PAD_PX)\n",
        "            ms = morph(ms, OPEN_K, CLOSE_K, MERGE_PAD_PX)\n",
        "            mb = morph(mb, OPEN_K, CLOSE_K, MERGE_PAD_PX)\n",
        "            mr = morph(mr, OPEN_K, CLOSE_K, MERGE_PAD_PX)\n",
        "            mg = morph(mg, OPEN_K, CLOSE_K, MERGE_PAD_PX)\n",
        "\n",
        "            area_min = int(MIN_AREA_FRAC * W0 * H0)\n",
        "\n",
        "            def comps_to_boxes(mask, prob_full):\n",
        "                num, lbl, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)\n",
        "                boxes=[]; scores=[]\n",
        "                for cid in range(1, num):\n",
        "                    x,y,w,h,area = stats[cid]\n",
        "                    if area < area_min:\n",
        "                        continue\n",
        "                    boxes.append([x,y,x+w,y+h])\n",
        "                    roi = (slice(y,y+h), slice(x,x+w))\n",
        "                    scores.append(float(np.mean(prob_full[roi])))\n",
        "                return boxes, scores\n",
        "\n",
        "            bw,  sw  = comps_to_boxes(mw, pw)\n",
        "            bv,  sv  = comps_to_boxes(mv, pv)\n",
        "            bs,  ss  = comps_to_boxes(ms, ps)\n",
        "            bb,  sb  = comps_to_boxes(mb, pb)\n",
        "            brd, srd = comps_to_boxes(mr, pr)\n",
        "            bgr, sgr = comps_to_boxes(mg, pg)\n",
        "\n",
        "            vis = frame_full.copy()\n",
        "\n",
        "            def draw_many(boxes, scores, color, name):\n",
        "                for b, sc in zip(boxes, scores):\n",
        "                    x1,y1,x2,y2 = map(int, b)\n",
        "                    cv2.rectangle(vis, (x1,y1), (x2,y2), color, 2)\n",
        "                    draw_label_smart(vis, x1,y1,x2,y2, f\"{name} {sc:.2f}\",\n",
        "                                     color=(int(color[0]),int(color[1]),int(color[2])))\n",
        "\n",
        "            draw_many(bw,  sw,  (0,0,255),   \"water\")\n",
        "            draw_many(bv,  sv,  (0,180,0),   \"vegetation\")\n",
        "            draw_many(bs,  ss,  (0,255,255), \"sky\")\n",
        "            draw_many(bb,  sb,  (255,0,0),   \"building\")\n",
        "            draw_many(brd, srd, (128,0,128), \"road\")\n",
        "            draw_many(bgr, sgr, (0,255,128), \"grass\")\n",
        "\n",
        "            cv2.putText(\n",
        "                vis,\n",
        "                f\"W:{len(bw)} V:{len(bv)} S:{len(bs)} B:{len(bb)} R:{len(brd)} G:{len(bgr)} | s={PROCESS_SCALE} | gamma={FG_SUPPRESS_GAMMA}\",\n",
        "                (12, H0-12),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2\n",
        "            )\n",
        "\n",
        "            writer.write(vis)\n",
        "            i += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "    cap.release()\n",
        "    writer.release()\n",
        "    elapsed = time.time() - t0\n",
        "    print(f\"[OK] BG boxes saved -> {out_path} ({i} frames in {elapsed:.1f}s)\")\n",
        "\n",
        "print(\"[INFO] Processing functions loaded successfully\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ELJUZJeqF-ek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1298e127-caa9-4395-adcb-192c143cd635"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Processing functions loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ============================================================\n",
        "# # FINAL RUNNER (RUN STAGES + SAVES + DOWNLOADS)\n",
        "# # ============================================================\n",
        "\n",
        "# if RUN_DENOISING:\n",
        "#     denoised_video = \"/content/video_denoised.mp4\"\n",
        "#     run_video_denoising(INPUT_VIDEO, denoised_video)\n",
        "#     PROCESSING_VIDEO = denoised_video\n",
        "# else:\n",
        "#     PROCESSING_VIDEO = INPUT_VIDEO\n",
        "\n",
        "# stats = None\n",
        "\n",
        "# if RUN_TRACKING:\n",
        "#     stats = run_tracking_fullvideo(PROCESSING_VIDEO, save_to_drive=SAVE_TO_DRIVE)\n",
        "#     print(json.dumps(stats, indent=2))\n",
        "#     try:\n",
        "#         ok_pf = save_best_tracking_paper_frame(\"/content/paper_frame_tracking.jpg\")\n",
        "#         print(\"[paper] tracking still saved:\", ok_pf)\n",
        "#     except Exception as e:\n",
        "#         print(\"[paper] tracking still failed:\", e)\n",
        "\n",
        "# if RUN_OUTLINES:\n",
        "#     run_outlines(PROCESSING_VIDEO, \"/content/outlined_output.mp4\")\n",
        "#     try:\n",
        "#         ok_pf2 = _save_frame_at_ratio(\"/content/outlined_output.mp4\",\n",
        "#                                       \"/content/paper_frame_outlines.jpg\",\n",
        "#                                       ratio=0.5)\n",
        "#         print(\"[paper] outlines still saved:\", ok_pf2)\n",
        "#     except Exception as e:\n",
        "#         print(\"[paper] outlines still failed:\", e)\n",
        "\n",
        "# if RUN_BG_ENSEMBLE:\n",
        "#     run_bg_ensemble(PROCESSING_VIDEO, \"/content/bg_boxes_clean_colors.mp4\")\n",
        "#     try:\n",
        "#         ok_pf3 = _save_frame_at_ratio(\"/content/bg_boxes_clean_colors.mp4\",\n",
        "#                                       \"/content/paper_frame_bg.jpg\",\n",
        "#                                       ratio=0.5)\n",
        "#         print(\"[paper] bg still saved:\", ok_pf3)\n",
        "#     except Exception as e:\n",
        "#         print(\"[paper] bg still failed:\", e)\n",
        "\n",
        "# print(\"\\n=== ENVIRONMENT PROOF ===\")\n",
        "# import ultralytics, deep_sort_realtime\n",
        "# print(\"Python:\", sys.version)\n",
        "# print(\"Torch:\", torch.__version__)\n",
        "# print(\"Torch CUDA available:\", torch.cuda.is_available())\n",
        "# print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
        "# print(\"Ultralytics version:\", ultralytics.__version__)\n",
        "# print(\"OpenCV version:\", cv2.__version__)\n",
        "# print(\"deep_sort_realtime version:\", deep_sort_realtime.__version__)\n",
        "\n",
        "# print(\"\\nPreparing downloads...\")\n",
        "# def try_dl(p):\n",
        "#     if os.path.exists(p):\n",
        "#         try:\n",
        "#             files.download(p)\n",
        "#         except Exception as e:\n",
        "#             print(\"download failed:\", p, e)\n",
        "\n",
        "# for p in [\n",
        "#     \"/content/video_denoised.mp4\",\n",
        "#     \"/content/run_stats.json\",\n",
        "#     \"/content/debug_frame.jpg\",\n",
        "#     \"/content/debug_frame_midpoint.jpg\",\n",
        "#     \"/content/debug_frame_end.jpg\",\n",
        "#     \"/content/output_tracked.mp4\",\n",
        "#     \"/content/outlined_output.mp4\",\n",
        "#     \"/content/bg_boxes_clean_colors.mp4\",\n",
        "#     \"/content/paper_frame_tracking.jpg\",\n",
        "#     \"/content/paper_frame_outlines.jpg\",\n",
        "#     \"/content/paper_frame_bg.jpg\",\n",
        "# ]:\n",
        "#     try_dl(p)\n",
        "\n",
        "# print(\"\\n✅ PIPELINE COMPLETE!\")\n",
        "# print(\"=\" * 60)\n",
        "# print(\"OUTPUTS:\")\n",
        "# if RUN_DENOISING:\n",
        "#     print(\"  - Denoised video (copied): /content/video_denoised.mp4\")\n",
        "# if RUN_TRACKING:\n",
        "#     print(\"  - Tracked video: /content/output_tracked.mp4\")\n",
        "# if RUN_OUTLINES:\n",
        "#     print(\"  - Outlined video: /content/outlined_output.mp4\")\n",
        "# if RUN_BG_ENSEMBLE:\n",
        "#     print(\"  - BG detection (6 classes): /content/bg_boxes_clean_colors.mp4\")\n",
        "# print(\"=\" * 60)\n"
      ],
      "metadata": {
        "id": "7NHTqOOHwVB4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U gradio\n",
        "\n",
        "import os, shutil, time, socket, threading\n",
        "import gradio as gr\n",
        "\n",
        "# ============================================================\n",
        "# NEURO_VISION — Gradio (Colab-friendly uploader + live logs)\n",
        "# Outputs: 4 tiles (Denoise, Tracking/FG, Outlines, Background)\n",
        "# ============================================================\n",
        "\n",
        "# ---- 1) Choose a free port (fixes \"Cannot find empty port 7860\") ----\n",
        "def find_free_port(start=7860, tries=50):\n",
        "    for p in range(start, start + tries):\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            try:\n",
        "                s.bind((\"0.0.0.0\", p))\n",
        "                return p\n",
        "            except OSError:\n",
        "                continue\n",
        "    return start + tries\n",
        "\n",
        "SERVER_PORT = find_free_port(7860, 50)\n",
        "\n",
        "# ---- 2) Live log capture ----\n",
        "class LiveLogger:\n",
        "    def __init__(self):\n",
        "        self.lines = []\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def log(self, *args):\n",
        "        msg = \" \".join(str(a) for a in args)\n",
        "        with self.lock:\n",
        "            self.lines.append(msg)\n",
        "        print(msg)\n",
        "\n",
        "    def get_text(self):\n",
        "        with self.lock:\n",
        "            return \"\\n\".join(self.lines[-350:])\n",
        "\n",
        "LOGGER = LiveLogger()\n",
        "\n",
        "# ============================================================\n",
        "# 3) OPTIONAL: Safe H.264 re-encode (only if helper exists)\n",
        "#    If your notebook already defines ffmpeg_to_h264(), we use it.\n",
        "# ============================================================\n",
        "def _maybe_h264(src_path, dst_path, fps=25.0):\n",
        "    if not src_path or not os.path.exists(src_path):\n",
        "        return None\n",
        "    if \"ffmpeg_to_h264\" in globals() and callable(globals()[\"ffmpeg_to_h264\"]):\n",
        "        try:\n",
        "            globals()[\"ffmpeg_to_h264\"](src_path, dst_path, fps)\n",
        "            return dst_path if os.path.exists(dst_path) else src_path\n",
        "        except Exception:\n",
        "            return src_path\n",
        "    return src_path\n",
        "\n",
        "# ============================================================\n",
        "# 4) Pipeline runner\n",
        "#    IMPORTANT: This uses YOUR existing notebook functions:\n",
        "#      - run_video_denoising(in_path, out_path)\n",
        "#      - run_tracking_fullvideo(in_path, save_to_drive=False)  [your signature]\n",
        "#      - run_outlines(in_path, out_path)                      [your signature]\n",
        "#      - run_bg_ensemble(in_path, out_path)                   [your signature]\n",
        "#\n",
        "#    And your notebook save paths are:\n",
        "#      /content/video_denoised.mp4\n",
        "#      /content/output_tracked.mp4\n",
        "#      /content/outlined_output.mp4\n",
        "#      /content/bg_boxes_clean_colors.mp4\n",
        "# ============================================================\n",
        "def run_pipeline(video_path, do_denoise, do_fg, do_outline, do_bg):\n",
        "    LOGGER.lines = []\n",
        "    t0 = time.time()\n",
        "\n",
        "    # stable input copy\n",
        "    in_path = \"/content/input_video.mp4\"\n",
        "    shutil.copy(video_path, in_path)\n",
        "    LOGGER.log(\"[INFO] Input copied ->\", in_path)\n",
        "\n",
        "    # set your global switches (if your notebook uses them)\n",
        "    global RUN_DENOISING, RUN_TRACKING, RUN_OUTLINES, RUN_BG_ENSEMBLE\n",
        "    RUN_DENOISING   = bool(do_denoise)\n",
        "    RUN_TRACKING    = bool(do_fg)\n",
        "    RUN_OUTLINES    = bool(do_outline)\n",
        "    RUN_BG_ENSEMBLE = bool(do_bg)\n",
        "\n",
        "    out_denoise = None\n",
        "    out_track   = None\n",
        "    out_outline = None\n",
        "    out_bg      = None\n",
        "\n",
        "    # -----------------------------\n",
        "    # STAGE 1: Denoising\n",
        "    # -----------------------------\n",
        "    proc_in = in_path\n",
        "    if RUN_DENOISING:\n",
        "        denoised = \"/content/video_denoised.mp4\"\n",
        "        LOGGER.log(\"[DENOISING] starting...\")\n",
        "        run_video_denoising(in_path, denoised)\n",
        "        proc_in = denoised\n",
        "        out_denoise = denoised\n",
        "        LOGGER.log(\"[DENOISING] done ->\", denoised)\n",
        "\n",
        "        out_denoise = _maybe_h264(out_denoise, \"/content/video_denoised_h264.mp4\", 25.0)\n",
        "        if out_denoise and out_denoise.endswith(\"_h264.mp4\"):\n",
        "            proc_in = out_denoise\n",
        "            LOGGER.log(\"[DENOISING] re-encoded ->\", out_denoise)\n",
        "\n",
        "    # -----------------------------\n",
        "    # STAGE 2: Foreground / Tracking\n",
        "    # -----------------------------\n",
        "    if RUN_TRACKING:\n",
        "        LOGGER.log(\"[TRACKING] starting...\")\n",
        "        stats = run_tracking_fullvideo(proc_in, save_to_drive=False)\n",
        "        # your function may return dict with output_file / input_fps\n",
        "        out_track = stats.get(\"output_file\") if isinstance(stats, dict) else \"/content/output_tracked.mp4\"\n",
        "        fps = stats.get(\"input_fps\", 25.0) if isinstance(stats, dict) else 25.0\n",
        "        if not out_track:\n",
        "            out_track = \"/content/output_tracked.mp4\"\n",
        "        LOGGER.log(\"[TRACKING] done ->\", out_track)\n",
        "\n",
        "        out_track = _maybe_h264(out_track, \"/content/output_tracked_h264.mp4\", fps)\n",
        "        if out_track and out_track.endswith(\"_h264.mp4\"):\n",
        "            LOGGER.log(\"[TRACKING] re-encoded ->\", out_track)\n",
        "\n",
        "    # -----------------------------\n",
        "    # STAGE 3: Contour Outlines\n",
        "    # (your notebook prints /content/outlined_output.mp4)\n",
        "    # -----------------------------\n",
        "    if RUN_OUTLINES:\n",
        "        LOGGER.log(\"[OUTLINES] starting...\")\n",
        "        outlined = \"/content/outlined_output.mp4\"\n",
        "        run_outlines(proc_in, outlined)\n",
        "        out_outline = outlined\n",
        "        LOGGER.log(\"[OUTLINES] done ->\", out_outline)\n",
        "\n",
        "        out_outline = _maybe_h264(out_outline, \"/content/outlined_output_h264.mp4\", 25.0)\n",
        "        if out_outline and out_outline.endswith(\"_h264.mp4\"):\n",
        "            LOGGER.log(\"[OUTLINES] re-encoded ->\", out_outline)\n",
        "\n",
        "    # -----------------------------\n",
        "    # STAGE 4: Background Ensemble\n",
        "    # (your notebook prints /content/bg_boxes_clean_colors.mp4)\n",
        "    # -----------------------------\n",
        "    if RUN_BG_ENSEMBLE:\n",
        "        LOGGER.log(\"[BG] starting...\")\n",
        "        bg_out = \"/content/bg_boxes_clean_colors.mp4\"\n",
        "        run_bg_ensemble(proc_in, bg_out)\n",
        "        out_bg = bg_out\n",
        "        LOGGER.log(\"[BG] done ->\", out_bg)\n",
        "\n",
        "        out_bg = _maybe_h264(out_bg, \"/content/bg_boxes_h264.mp4\", 25.0)\n",
        "        if out_bg and out_bg.endswith(\"_h264.mp4\"):\n",
        "            LOGGER.log(\"[BG] re-encoded ->\", out_bg)\n",
        "\n",
        "    LOGGER.log(\"[DONE] total_sec =\", round(time.time() - t0, 2))\n",
        "    return out_denoise, out_track, out_outline, out_bg\n",
        "\n",
        "# ============================================================\n",
        "# 5) Generator fn: keeps connection alive by yielding logs\n",
        "# ============================================================\n",
        "def gradio_runner(video, do_denoise, do_fg, do_outline, do_bg):\n",
        "    if video is None:\n",
        "        yield None, None, None, None, \"Upload a video first.\"\n",
        "        return\n",
        "\n",
        "    video_path = video  # in your working reference, this is a filepath string\n",
        "\n",
        "    result = {\"denoise\": None, \"track\": None, \"outline\": None, \"bg\": None, \"done\": False, \"err\": None}\n",
        "\n",
        "    def _worker():\n",
        "        try:\n",
        "            d, t, o, b = run_pipeline(video_path, do_denoise, do_fg, do_outline, do_bg)\n",
        "            result[\"denoise\"], result[\"track\"], result[\"outline\"], result[\"bg\"] = d, t, o, b\n",
        "        except Exception as e:\n",
        "            result[\"err\"] = str(e)\n",
        "        finally:\n",
        "            result[\"done\"] = True\n",
        "\n",
        "    th = threading.Thread(target=_worker, daemon=True)\n",
        "    th.start()\n",
        "\n",
        "    while not result[\"done\"]:\n",
        "        yield result[\"denoise\"], result[\"track\"], result[\"outline\"], result[\"bg\"], LOGGER.get_text()\n",
        "        time.sleep(0.7)\n",
        "\n",
        "    if result[\"err\"]:\n",
        "        yield None, None, None, None, LOGGER.get_text() + \"\\n\\n[ERROR] \" + result[\"err\"]\n",
        "    else:\n",
        "        yield result[\"denoise\"], result[\"track\"], result[\"outline\"], result[\"bg\"], LOGGER.get_text()\n",
        "\n",
        "# ============================================================\n",
        "# 6) UI — NEURO_VISION (2x2 outputs + logs)\n",
        "# ============================================================\n",
        "with gr.Blocks(css=\"\"\"\n",
        ".gradio-container { background: #0f0f0f !important; color: #fff !important; }\n",
        "h1, h2, h3 { color: #22c55e !important; }\n",
        "\"\"\") as demo:\n",
        "\n",
        "    gr.HTML(\"<h1 style='text-align:center; font-family:monospace;'>🎬 NEURO_VISION</h1>\")\n",
        "    gr.Markdown(\"**Colab-friendly uploader + live logs + 4 output tiles (2×2)**\")\n",
        "\n",
        "    with gr.Row():\n",
        "        video_in = gr.Video(label=\"Upload Video\", format=\"mp4\")  # keep reference uploader behavior\n",
        "        with gr.Column():\n",
        "            opt_denoise = gr.Checkbox(value=True,  label=\"✨ Denoising\")\n",
        "            opt_fg      = gr.Checkbox(value=True,  label=\"🎯 Foreground/Tracking (Stage A)\")\n",
        "            opt_outline = gr.Checkbox(value=True,  label=\"📐 Contour Outlining (Stage B)\")\n",
        "            opt_bg      = gr.Checkbox(value=True,  label=\"🎨 Background (Stage C)\")\n",
        "            run_btn     = gr.Button(\"▶ RUN NEURO_VISION\", variant=\"primary\")\n",
        "\n",
        "    logs = gr.Textbox(label=\"Live Logs\", lines=20)\n",
        "\n",
        "    gr.Markdown(\"### Outputs (2×2)\")\n",
        "    with gr.Row():\n",
        "        out_denoise = gr.Video(label=\"Denoised\", format=\"mp4\")\n",
        "        out_track   = gr.Video(label=\"Tracking / FG\", format=\"mp4\")\n",
        "    with gr.Row():\n",
        "        out_outline = gr.Video(label=\"Outlines\", format=\"mp4\")\n",
        "        out_bg      = gr.Video(label=\"Background\", format=\"mp4\")\n",
        "\n",
        "    run_btn.click(\n",
        "        fn=gradio_runner,\n",
        "        inputs=[video_in, opt_denoise, opt_fg, opt_outline, opt_bg],\n",
        "        outputs=[out_denoise, out_track, out_outline, out_bg, logs],\n",
        "        concurrency_limit=1\n",
        "    )\n",
        "\n",
        "# Queue (version-safe)\n",
        "try:\n",
        "    demo.queue(max_size=8, default_concurrency_limit=1)\n",
        "except TypeError:\n",
        "    demo.queue()\n",
        "\n",
        "demo.launch(share=True, server_port=SERVER_PORT, debug=True, max_threads=2)\n"
      ],
      "metadata": {
        "id": "UVKKhg2V6tzi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "a0d5c1d9-60e9-4070-99e7-cd2146561654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.0/23.0 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hColab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://cd471127b0f5377a00.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cd471127b0f5377a00.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
